## Index

1. 선형회귀모형의 고전적 가정
2. 반응변수와 설명변수 간 선형관계의 이해
3. 좋은 성질을 가지는 최소자승추정량의 이해

<hr></br>

## 선형회귀모형의 고전적 가정

- `A.1` **반응변수와 설명변수의 선형성 가정**

    >The Linear Model is Correctly Specified.

- `A.2` **설명변수의 비확률성 가정 혹은 설명변수 간 통계적 독립성 가정**

    >$X_i$ is Non-Random For Every $i=1,\cdots,n$

- `A.3` **오차항의 기대값에 관한 가정**

    >$E(\varepsilon_i)=0$ For Every $i=1,\cdots,n$

- `A.4` **오차항의 등분산성(Homoskedasticity) 가정**

    >$Var(\varepsilon_i)=\sigma^2$ For Every $i=1,\cdots,n$

- `A.5` **관측치 간 오차항의 자기상관 없음(No Autocorrelation) 가정**

    >$Cov(\varepsilon_i, \varepsilon_j)=0$ If $i \ne j$

</br>

## 반응변수와 설명변수 간 선형관계의 이해

- **가중치 $\beta_1$ 에 관한 최소자승추정량 $\hat{\beta_1}$ 은 다음과 같음**

    $$\begin{aligned}
    \hat{\beta_1}
    &= \displaystyle\frac{Cov(X,Y)}{Var(X)} \\
    &= \displaystyle\frac{\displaystyle\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})}{\displaystyle\sum_{i=1}^{n}(X_i-\overline{X})^2}
    \end{aligned}$$

- **분자를 다음과 같이 이해할 수 있음**

    $$\begin{aligned}
    \displaystyle\sum_{i=1}^{n}(X_i-\overline{X})(Y_i-\overline{Y})
    &= \displaystyle\sum_{i=1}^{n}(X_i-\overline{X})Y_i-\displaystyle\sum_{i=1}^{n}(X_i-\overline{X})\overline{Y} \\
    &= \displaystyle\sum_{i=1}^{n}(X_i-\overline{X})Y_i - \overline{Y} \times \displaystyle\sum_{i=1}^{n}(X_i-\overline{X}) \\
    &= \displaystyle\sum_{i=1}^{n}(X_i-\overline{X})Y_i\;(\because \displaystyle\sum_{i=1}^{n}(X_i-\overline{X}) = 0)
    \end{aligned}$$

- 즉, $\hat{\beta_1}$ 은 $Y_i$ 들의 **선형조합(Linear Combination)**

    $$\begin{aligned}
    \hat{\beta_1}
    &= \displaystyle\frac{\displaystyle\sum_{i=1}^{n}(X_i-\overline{X})Y_i}{\displaystyle\sum_{i=1}^{n}(X_i-\overline{X})^2} \\
    &= w_1Y_1 + w_2Y_2 + \cdots + w_nY_n \\
    \\
    (w_i
    &= \displaystyle\frac{(X_i-\overline{X})}{\displaystyle\sum_{j=1}^{n}(X_j-\overline{X})^2},\;i=1,2,\cdots,n)
    \end{aligned}$$

- **$\hat{\beta_1}$ 에 대하여 다음이 성립함** (증명 생략)
    - $\displaystyle\sum_{i=1}^{n}w_i=0$
    - $\displaystyle\sum_{i=1}^{n}w_iX_i=1$
    - $\displaystyle\sum_{i=1}^{n}w_i^2=\displaystyle\frac{1}{\displaystyle\sum_{i=1}^{n}(X_i-\overline{X})^2}$

- **따라서 최소자승추정량 $\hat{\beta_1}$ 과 그 모수 $\beta_1$ 사이에는 다음이 성립함**

    $$\begin{aligned}
    \hat{\beta_1}
    &= \displaystyle\sum_{i=1}^{n}w_iY_i \\
    &= \displaystyle\sum_{i=1}^{n}w_i(\beta_0+\beta_1X_i+\varepsilon_i) \\
    &= \beta_1 + \displaystyle\sum_{i=1}^{n}w_i\varepsilon_i
    \end{aligned}$$

</br>

## 좋은 성질을 가지는 최소자승추정량의 이해

- **고전적 가정(Classical Assumptions) 하에서 최소자승추정량은 좋은 성질을 가짐**
    - **불편성(Unbiasedness)**

        $$
        Bias(\hat{\beta})=E(\hat{\beta}) - \beta=0
        $$

    - **효율성(Efficiency)**

        $$\begin{aligned}
        \min MSE(\hat{\beta})
        &= \min E((\hat{\beta}-\beta)^2) \\
        &= \min (Var(\hat{\beta}) + Bias(\hat{\beta})^2)
        \end{aligned}$$

- **불편성(Unbiasedness)**
    - **증명** ($\hat{\beta_0}$ 생략)

        $$\begin{aligned}
        E(\hat{\beta_1})
        &= E(\beta_1 + \displaystyle\sum_{i=1}^{n}w_i\varepsilon_i) \\
        &= E(\beta_1) + E(\displaystyle\sum_{i=1}^{n}w_i\varepsilon_i) \\
        &= \beta_1 + \displaystyle\sum_{i=1}^{n}E(w_i\varepsilon_i) \\
        &= \beta_1\;(\because A.2\;and\;A.3) \\
        \\
        \therefore Bias(\hat{\beta_1}) &= 0
        \end{aligned}$$

- **효율성(Efficiency)**
    - **Gauss-Markov Theorem** : 최소자승추정량 $\hat{\beta_1}, \hat{\beta_0}$ 은 그 모수 $\beta_1, \beta_0$ 의 선형불편추정량 중 가장 효율적인 추정량임

        >Given the classical assumptions, the OLS estimators are BLUE(Best Linear Unbiased Estimators). That is, They are the most efficient in the class of linear unbiased estimators.

    - **최소자승추정량의 분산**
        - $Var(\hat{\beta_1}) = \displaystyle\frac{\sigma^2}{\displaystyle\sum_{i}(X_i-\overline{X})^2}$
        - $Var(\hat{\beta_0}) = \displaystyle\frac{\sigma^2}{n}\displaystyle\frac{\displaystyle\sum_{i}X_i^2}{\displaystyle\sum_{i}(X_i-\overline{X})^2}$

    - **증명** ($\hat{\beta_0}$ 생략)
        $$\begin{aligned}
        Var(\hat{\beta_1})
        &= E[(\hat{\beta_1}-E(\hat{\beta_1}))]^2 \\
        &= E[(\hat{\beta_1}-\beta_1)]^2\;(\because\;Bias(\hat{\beta_1})=0) \\
        &= E(\displaystyle\sum_{i=1}^{n}w_i\varepsilon_i)^2 \\
        &= E(\displaystyle\sum_{i}w_i^2\varepsilon_i^2 + \displaystyle\sum_{i}\displaystyle\sum_{i\ne j}w_iw_j\varepsilon_i\varepsilon_j) \\
        &= \displaystyle\sum_{i}w_i^2E(\varepsilon_i^2)+ \displaystyle\sum_{i\ne j}w_iw_jE(\varepsilon_i\varepsilon_j) \\
        &= \displaystyle\sum_{i}w_i^2\sigma^2\;(\because A.2,\;A.3\;and\;A.5) \\
        &= \sigma^2\displaystyle\sum_{i}w_i^2 \\
        &= \displaystyle\frac{\sigma^2}{\displaystyle\sum_{i}(X_i-\overline{X})^2}
        \end{aligned}$$